{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d95035b",
   "metadata": {},
   "source": [
    "# The freMTPL2freq dataset\n",
    "\n",
    "Welcome to the glamorous world of motor insurance! In this workshop, you'll work with a real-world French motor third‑party liability dataset, a classic benchmark dataset for [claim frequency modeling](https://openacttexts.github.io/Loss-Data-Analytics/ChapFrequency-Modeling.html) (i.e., *how many claims do we expect per policy-year?*). The dataset comes from [CASdatasets](https://dutangc.github.io/CASdatasets/).\n",
    "\n",
    "Each row corresponds to a single insurance policy exposure period, with the following columns:\n",
    "\n",
    "- `IDpol`: The policy ID.\n",
    "- `ClaimNb`: Number of claims during the exposure period.\n",
    "- `Exposure`: The duration of the policy.\n",
    "- `VehPower`: The power of the car (ordered values).\n",
    "- `VehAge`: The vehicle age, in years\n",
    "- `DrivAge`: The driver age, in years (in France, people can drive a car at 18)\n",
    "- `BonusMalus`: Risk-based rating factor between 50 and 350 (threshold expected to be at 100).\n",
    "- `VehBrand`: The car brand (unknown categories).\n",
    "- `VehGas`: The car's fuel type (Diesel or Regular).\n",
    "- `Area`: discretized (binned) version of population density (from \"A\" for rural area to \"F\" for urban centre).\n",
    "- `Density`: The density of inhabitants (number of inhabitants per square-kilometer) of the city where the car driver lives in.\n",
    "- `Region`: The policy region in France.\n",
    "\n",
    "Below you'll find some possible starting points. Pick the level that best suits\n",
    "you, dig in, or ignore them and do your own thing. Happy coding!\n",
    "\n",
    "\n",
    "##### **Beginner**\n",
    "- Start by getting oriented with `.head()`, `.info()`, `.describe()`.\n",
    "    - How many rows/columns do you have?\n",
    "    - Which columns are numeric vs categorical?\n",
    "    - Are there missing values? If yes: where and how many?\n",
    "\n",
    "- Sanity-check the basics:\n",
    "    - What is total exposure in the dataset: `df[\"Exposure\"].sum()`?\n",
    "    - How many total claims: `df[\"ClaimNb\"].sum()`?\n",
    "    - What is the overall claim frequency (claims per year)?\n",
    "    - Hint: a good baseline is `total_claims / total_exposure`.\n",
    "\n",
    "- Claim counts are usually *very* skewed. Here are some questions you could answer:\n",
    "    - Plot the distribution of `ClaimNb`.\n",
    "    - What fraction of rows have `ClaimNb = 0`?\n",
    "    - What's the maximum `ClaimNb`? Inspect a few of the largest-claim rows.\n",
    "\n",
    "- Explore the categorical columns using `.value_counts()`:\n",
    "    - `VehBrand`, `VehGas`, `Region`, `VehPower`  \n",
    "    Which ones have many levels? Any very rare categories?\n",
    "\n",
    "- Build your first \"actuarial\" summary table:\n",
    "    - Compute claim rate (claim frequency per 1 unit exposure) by `VehGas` and by `Region`.\n",
    "    - Important hint: don't average `ClaimNb / Exposure` row-by-row at first, instead compute `sum(ClaimNb) / sum(Exposure)` within each group.\n",
    "\n",
    "\n",
    "\n",
    "##### **Intermediate**\n",
    "-  Try out creating a rate column, which would be the claim frequency per 1 unit exposure:\n",
    "    - Define `ClaimFreq = ClaimNb / Exposure`\n",
    "    - Before you do: check if any `Exposure` is 0 or extremely small. What would that do to `ClaimFreq`?\n",
    "\n",
    "- Make risk buckets (this dataset loves buckets) using [`pd.cut()`](https://pandas.pydata.org/docs/reference/api/pandas.cut.html):\n",
    "    - Bin `DrivAge` into age groups (e.g. 18-25, 26-35, etc.)\n",
    "    - Bin `VehAge` into \"new-ish\" vs \"old-ish\" (or more bins).\n",
    "\n",
    "- Build a mini \"pricing table\":\n",
    "    - For each of these, compute `sum(ClaimNb) / sum(Exposure)` and rank groups:\n",
    "        - `Region`\n",
    "        - `VehPower`\n",
    "        - `VehBrand` (any rare brands?)\n",
    "        - combinations like `(Region, VehGas)` or `(DrivAgeGroup, VehPower)`\n",
    "\n",
    "- Bonus/malus is meant to be informative, lets test that it is:\n",
    "    - Plot claim rate as a function of `BonusMalus` (bin it if it has many unique values).\n",
    "    - Is the relationship monotone? Are there weird plateaus or jumps?\n",
    "\n",
    "- Visual storytelling ideas:\n",
    "    - A heatmap of claim rates for `(DrivAgeGroup x VehPower)` or `(Region x VehGas)`.\n",
    "    - A \"risk profile\" plot: predicted/observed claim rate vs driver age group.\n",
    "\n",
    "\n",
    "##### **Advanced**\n",
    "- Try out building a proper frequency model (the canonical approach):\n",
    "    - Split into train/test (or use cross-validation).\n",
    "    - Fit a [Poisson GLM](https://en.wikipedia.org/wiki/Poisson_regression) with a log link and `Exposure` as an *offset*:\n",
    "        - Model `ClaimNb ~ features + offset(log(Exposure))`\n",
    "        - If you use `statsmodels`, look into [`GLM`](https://www.statsmodels.org/stable/glm.html).\n",
    "\n",
    "- Perhaps the claim frequency is not actually Poisson distributed here? A common problem is \"overdispersion\", i.e. that `Var(ClaimNb)` is much larger than `Mean(ClaimNb)`. Does this hold in our data, and does this contradict the Poisson distribution? You might compare Poisson vs [Negative Binomial](https://en.wikipedia.org/wiki/Negative_binomial_distribution) (and see if it helps in the modeling).\n",
    "\n",
    "- Another idea is to go nonlinear:\n",
    "    - Fit a tree/boosting model with a Poisson objective (e.g. [XGBoost](https://xgboost.readthedocs.io/en/stable/), [LightGBM](https://lightgbm.readthedocs.io/en/latest/Python-Intro.html), or [CatBoost](https://catboost.ai/docs/en/concepts/python-quickstart)).\n",
    "    - Handle categoricals carefully (one-hot, target encoding, or a model that natively supports categoricals).\n",
    "    - Don't forget exposure: use it as an offset (if supported) or as weights + careful target definition.\n",
    "\n",
    "- Interpretability (because \"why does the model think that?\"):\n",
    "    - For GLMs: interpret coefficients (and check multicollinearity).\n",
    "    - For boosting: try [SHAP](https://shap.readthedocs.io/en/latest/) and inspect whether effects make sense (e.g., BonusMalus, age, density).\n",
    "    - Cutting edge: try out the [Explainable Boosting Machine (EBM)](https://interpret.ml/docs/ebm.html), which is a boosting method known to give performance at par with SOTA boosting methods while remaining interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50bfc278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDpol</th>\n",
       "      <th>ClaimNb</th>\n",
       "      <th>Exposure</th>\n",
       "      <th>VehPower</th>\n",
       "      <th>VehAge</th>\n",
       "      <th>DrivAge</th>\n",
       "      <th>BonusMalus</th>\n",
       "      <th>VehBrand</th>\n",
       "      <th>VehGas</th>\n",
       "      <th>Area</th>\n",
       "      <th>Density</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>50</td>\n",
       "      <td>B12</td>\n",
       "      <td>Regular</td>\n",
       "      <td>D</td>\n",
       "      <td>1217</td>\n",
       "      <td>Rhone-Alpes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.77</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>50</td>\n",
       "      <td>B12</td>\n",
       "      <td>Regular</td>\n",
       "      <td>D</td>\n",
       "      <td>1217</td>\n",
       "      <td>Rhone-Alpes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>50</td>\n",
       "      <td>B12</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>B</td>\n",
       "      <td>54</td>\n",
       "      <td>Picardie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>50</td>\n",
       "      <td>B12</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>B</td>\n",
       "      <td>76</td>\n",
       "      <td>Aquitaine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>50</td>\n",
       "      <td>B12</td>\n",
       "      <td>Diesel</td>\n",
       "      <td>B</td>\n",
       "      <td>76</td>\n",
       "      <td>Aquitaine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDpol  ClaimNb  Exposure  VehPower  VehAge  DrivAge  BonusMalus VehBrand  \\\n",
       "0    1.0        1      0.10         5       0       55          50      B12   \n",
       "1    3.0        1      0.77         5       0       55          50      B12   \n",
       "2    5.0        1      0.75         6       2       52          50      B12   \n",
       "3   10.0        1      0.09         7       0       46          50      B12   \n",
       "4   11.0        1      0.84         7       0       46          50      B12   \n",
       "\n",
       "    VehGas Area  Density       Region  \n",
       "0  Regular    D     1217  Rhone-Alpes  \n",
       "1  Regular    D     1217  Rhone-Alpes  \n",
       "2   Diesel    B       54     Picardie  \n",
       "3   Diesel    B       76    Aquitaine  \n",
       "4   Diesel    B       76    Aquitaine  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('freMTPL2freq.csv')\n",
    "\n",
    "# Fler rader: om du har ytterligare en CSV med samma kolumner, slå ihop:\n",
    "# df_extra = pd.read_csv('sökväg/till/annan_fil.csv')\n",
    "# df = pd.concat([df, df_extra], ignore_index=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882b0cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-poisson-nloglik:0.56560\ttest-poisson-nloglik:0.56630\n",
      "[100]\ttrain-poisson-nloglik:0.30912\ttest-poisson-nloglik:0.30874\n",
      "[200]\ttrain-poisson-nloglik:0.22990\ttest-poisson-nloglik:0.22933\n",
      "[300]\ttrain-poisson-nloglik:0.20889\ttest-poisson-nloglik:0.20848\n",
      "[400]\ttrain-poisson-nloglik:0.20382\ttest-poisson-nloglik:0.20369\n",
      "[500]\ttrain-poisson-nloglik:0.20234\ttest-poisson-nloglik:0.20251\n",
      "[600]\ttrain-poisson-nloglik:0.20166\ttest-poisson-nloglik:0.20209\n",
      "[700]\ttrain-poisson-nloglik:0.20117\ttest-poisson-nloglik:0.20181\n",
      "[800]\ttrain-poisson-nloglik:0.20077\ttest-poisson-nloglik:0.20164\n",
      "[900]\ttrain-poisson-nloglik:0.20043\ttest-poisson-nloglik:0.20150\n",
      "[1000]\ttrain-poisson-nloglik:0.20013\ttest-poisson-nloglik:0.20139\n",
      "[1100]\ttrain-poisson-nloglik:0.19986\ttest-poisson-nloglik:0.20131\n",
      "[1200]\ttrain-poisson-nloglik:0.19960\ttest-poisson-nloglik:0.20124\n",
      "[1300]\ttrain-poisson-nloglik:0.19936\ttest-poisson-nloglik:0.20118\n",
      "[1400]\ttrain-poisson-nloglik:0.19911\ttest-poisson-nloglik:0.20109\n",
      "[1500]\ttrain-poisson-nloglik:0.19886\ttest-poisson-nloglik:0.20103\n",
      "[1600]\ttrain-poisson-nloglik:0.19863\ttest-poisson-nloglik:0.20098\n",
      "[1700]\ttrain-poisson-nloglik:0.19839\ttest-poisson-nloglik:0.20091\n",
      "[1800]\ttrain-poisson-nloglik:0.19818\ttest-poisson-nloglik:0.20087\n",
      "[1900]\ttrain-poisson-nloglik:0.19797\ttest-poisson-nloglik:0.20081\n",
      "[2000]\ttrain-poisson-nloglik:0.19777\ttest-poisson-nloglik:0.20079\n",
      "[2100]\ttrain-poisson-nloglik:0.19757\ttest-poisson-nloglik:0.20076\n",
      "[2200]\ttrain-poisson-nloglik:0.19737\ttest-poisson-nloglik:0.20072\n",
      "[2300]\ttrain-poisson-nloglik:0.19718\ttest-poisson-nloglik:0.20072\n",
      "[2400]\ttrain-poisson-nloglik:0.19699\ttest-poisson-nloglik:0.20069\n",
      "[2500]\ttrain-poisson-nloglik:0.19679\ttest-poisson-nloglik:0.20066\n",
      "[2589]\ttrain-poisson-nloglik:0.19664\ttest-poisson-nloglik:0.20066\n",
      "Exempel på förutsägelser (första 10): [0.04262117 0.05374344 0.01064515 0.01551917 0.06325798 0.04873801\n",
      " 0.03664767 0.03033949 0.0198891  0.00577563]\n",
      "MAE (test): 0.0960611528508138\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "# Feature engineering: åldersgrupper\n",
    "df_ = df.copy()\n",
    "df_['DrivAgeGroup'] = pd.cut(df_['DrivAge'], bins=[17, 25, 35, 45, 55, 65, 100], labels=False).astype(float).fillna(0)\n",
    "df_['VehAgeGroup'] = pd.cut(df_['VehAge'], bins=[-1, 0, 5, 10, 50], labels=False).astype(float).fillna(0)\n",
    "\n",
    "# Stratifierad split (samma andel 0/1+ skador i train och test)\n",
    "strat = (df_['ClaimNb'] >= 1).astype(int)\n",
    "train_idx, test_idx = train_test_split(df_.index, test_size=0.1, random_state=42, stratify=strat)\n",
    "train_df = df_.loc[train_idx].copy()\n",
    "test_df = df_.loc[test_idx].copy()\n",
    "\n",
    "# Target encoding med smoothing (endast på train)\n",
    "global_rate = train_df['ClaimNb'].sum() / train_df['Exposure'].sum()\n",
    "smooth = 100\n",
    "def target_encode(df_fit, df_apply, col):\n",
    "    agg = df_fit.groupby(col).apply(lambda g: (g['ClaimNb'].sum() + global_rate * smooth) / (g['Exposure'].sum() + smooth), include_groups=False)\n",
    "    return df_apply[col].map(agg).fillna(global_rate).values\n",
    "\n",
    "train_df['Region_rate'] = target_encode(train_df, train_df, 'Region')\n",
    "test_df['Region_rate'] = target_encode(train_df, test_df, 'Region')\n",
    "train_df['VehBrand_rate'] = target_encode(train_df, train_df, 'VehBrand')\n",
    "test_df['VehBrand_rate'] = target_encode(train_df, test_df, 'VehBrand')\n",
    "train_df['Area_rate'] = target_encode(train_df, train_df, 'Area')\n",
    "test_df['Area_rate'] = target_encode(train_df, test_df, 'Area')\n",
    "\n",
    "# Log-Density och Density-grupp (rural / mellan / urban)\n",
    "train_df['log_Density'] = np.log1p(train_df['Density'])\n",
    "test_df['log_Density'] = np.log1p(test_df['Density'])\n",
    "density_bins = [0, 4, 7, 15]\n",
    "train_df['DensityGroup'] = np.digitize(train_df['log_Density'], density_bins).clip(0, len(density_bins)).astype(float)\n",
    "test_df['DensityGroup'] = np.digitize(test_df['log_Density'], density_bins).clip(0, len(density_bins)).astype(float)\n",
    "\n",
    "# Numeriska + target-encodade + one-hot kategorier\n",
    "num_cols = ['VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'Density', 'log_Density', 'DensityGroup', 'DrivAgeGroup', 'VehAgeGroup', 'Region_rate', 'VehBrand_rate', 'Area_rate']\n",
    "cat_cols = ['VehBrand', 'VehGas', 'Area', 'Region']\n",
    "X_cat_train = pd.get_dummies(train_df[cat_cols], drop_first=True)\n",
    "X_cat_test = pd.get_dummies(test_df[cat_cols], drop_first=True)\n",
    "X_cat_test = X_cat_test.reindex(columns=X_cat_train.columns, fill_value=0)\n",
    "X_train = pd.concat([train_df[num_cols].astype(float), X_cat_train], axis=1)\n",
    "X_test = pd.concat([test_df[num_cols].astype(float), X_cat_test], axis=1)\n",
    "y_train = train_df['ClaimNb'].values\n",
    "y_test = test_df['ClaimNb'].values\n",
    "off_train = np.log(train_df['Exposure'].values)\n",
    "off_test = np.log(test_df['Exposure'].values)\n",
    "\n",
    "\n",
    "\n",
    "# DMatrix med base_margin = log(Exposure) som offset (Poisson)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, base_margin=off_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test, base_margin=off_test)\n",
    "\n",
    "params = {\n",
    "    'objective': 'count:poisson',\n",
    "    'eval_metric': 'poisson-nloglik',\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.02,\n",
    "    'min_child_weight': 35,\n",
    "    'subsample': 0.85,\n",
    "    'colsample_bytree': 0.85,\n",
    "    'reg_alpha': 0.15,\n",
    "    'reg_lambda': 1.2,\n",
    "}\n",
    "\n",
    "num_rounds = 12000\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=num_rounds,\n",
    "    evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "    early_stopping_rounds=80,\n",
    "    verbose_eval=100\n",
    ")\n",
    "\n",
    "# Förutsägelse = exposure * rate (XGBoost ger redan det när base_margin sätts)\n",
    "pred_claims = model.predict(dtest)\n",
    "print('Exempel på förutsägelser (första 10):', pred_claims[:10])\n",
    "print('MAE (test):', np.abs(pred_claims - y_test).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
